{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlXdfNQTjxxOrhIaiat76O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install segmentation-models-pytorch\n","!python -m pip install pyyaml==5.1\n","# Detectron2 has not released pre-built binaries for the latest pytorch (https://github.com/facebookresearch/detectron2/issues/4053)\n","# so we install from source instead. This takes a few minutes.\n","!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"],"metadata":{"id":"ax0glxxWlLln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imports\n","import os\n","import torch\n","import detectron2\n","import torchvision\n","from PIL import Image\n","from torch.utils.data import Dataset, random_split\n","import numpy as np\n","import segmentation_models_pytorch as smp\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from detectron2.data import DatasetCatalog\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog, DatasetMapper\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","import random\n","import time\n","from collections import defaultdict\n"],"metadata":{"id":"UoweMXPZc68O"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1FHZOX_EaWsa","executionInfo":{"status":"ok","timestamp":1653961933542,"user_tz":420,"elapsed":820,"user":{"displayName":"Poojan Pandya","userId":"12302368320074108368"}},"outputId":"51169ceb-3580-42e5-bc3d-bc05672355ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# Detectron 2"],"metadata":{"id":"L_v2gL2L6laQ"}},{"cell_type":"code","source":["IMAGES_PATH = '/content/drive/MyDrive/CS231N Project/Radiographs/'"],"metadata":{"id":"AgaWzm-Zb2Pc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MASKS_PATH = '/content/drive/MyDrive/CS231N Project/Segmentation/teeth_mask/'"],"metadata":{"id":"tVhT4GikYpar"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def data_split():\n","    # there are 1000 total images\n","    random.seed(42)\n","    images = os.listdir(IMAGES_PATH)\n","    random.shuffle(images)\n","    train = images[:800]\n","    val = images[800:900]\n","    test = images[900:]\n","    return train, val, test"],"metadata":{"id":"XwHsOJlZGrp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, val, test = data_split()\n","\n","def teeth_segmentation_train():\n","    \"\"\"\n","    Sets up data for use in detectron2\n","    Returns list[dict] with fields for semantic segmentation\n","    \"\"\"\n","    res = []  # list[dict]\n","    for filename in sorted(train):\n","        info = {}\n","        info['file_name'] = os.path.join(IMAGES_PATH, filename)\n","        info['height'] = 840\n","        info['width'] = 1615\n","        info['image_id'] = filename.split('.')[0]  # get just the number portion of the filename\n","        info['sem_seg_file_name'] = os.path.join(MASKS_PATH, filename.replace('JPG', 'jpg'))\n","        res.append(info)\n","    return res\n","\n","DatasetCatalog.register(\"teeth_segmentation_train\", teeth_segmentation_train)\n","\n","def teeth_segmentation_val():\n","    \"\"\"\n","    Sets up data for use in detectron2\n","    Returns list[dict] with fields for semantic segmentation\n","    \"\"\"\n","    res = []  # list[dict]\n","    for filename in sorted(val):\n","        info = {}\n","        info['file_name'] = os.path.join(IMAGES_PATH, filename)\n","        info['height'] = 840\n","        info['width'] = 1615\n","        info['image_id'] = filename.split('.')[0]  # get just the number portion of the filename\n","        info['sem_seg_file_name'] = os.path.join(MASKS_PATH, filename.replace('JPG', 'jpg'))\n","        res.append(info)\n","    return res\n","\n","DatasetCatalog.register(\"teeth_segmentation_val\", teeth_segmentation_val)"],"metadata":{"id":"3ijy0Zze6qo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from detectron2.engine import DefaultTrainer\n","\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.DATASETS.TRAIN = (\"teeth_segmentation_train\",)\n","cfg.DATASETS.TEST = ()\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n","cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people\n","cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n","cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n","cfg.SOLVER.STEPS = []        # do not decay learning rate\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n","# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg) \n","trainer.resume_or_load(resume=False)\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"CvGajl-TCkup","executionInfo":{"status":"error","timestamp":1653961949356,"user_tz":420,"elapsed":15819,"user":{"displayName":"Poojan Pandya","userId":"12302368320074108368"}},"outputId":"3105b9a0-211e-42ad-8b28-6f77d3f582b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32m[05/31 01:52:21 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[32m[05/31 01:52:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[05/31 01:52:21 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[05/31 01:52:21 d2.data.common]: \u001b[0mSerializing 800 elements to byte tensors and concatenating them all ...\n","\u001b[32m[05/31 01:52:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.17 MiB\n"]},{"output_type":"stream","name":"stderr","text":["Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n","Some model parameters or buffers are not found in the checkpoint:\n","\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32m[05/31 01:52:24 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n","\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[05/31 01:52:29 d2.engine.train_loop]: \u001b[0mException during training:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/detectron2/engine/train_loop.py\", line 149, in train\n","    self.run_step()\n","  File \"/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\", line 494, in run_step\n","    self._trainer.run_step()\n","  File \"/usr/local/lib/python3.7/dist-packages/detectron2/engine/train_loop.py\", line 273, in run_step\n","    loss_dict = self.model(data)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/rcnn.py\", line 161, in forward\n","    proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/detectron2/modeling/proposal_generator/rpn.py\", line 470, in forward\n","    assert gt_instances is not None, \"RPN requires gt_instances in training!\"\n","AssertionError: RPN requires gt_instances in training!\n","\u001b[32m[05/31 01:52:29 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:04 (0:00:00 on hooks)\n","\u001b[32m[05/31 01:52:29 d2.utils.events]: \u001b[0m iter: 0    lr: N/A  max_mem: 1531M\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-4e2deecad657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mOrderedDict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \"\"\"\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPECTED_RESULTS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             assert hasattr(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# self.iter == max_iter can be used by `after_train` to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdo\u001b[0m \u001b[0msomething\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myou\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mwrap\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \"\"\"\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_generator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m\"proposals\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/modeling/proposal_generator/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mgt_instances\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RPN requires gt_instances in training!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mgt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_and_sample_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             losses = self.losses(\n","\u001b[0;31mAssertionError\u001b[0m: RPN requires gt_instances in training!"]}]},{"cell_type":"markdown","source":["# Custom Data Loader"],"metadata":{"id":"m7GekaMY6bfC"}},{"cell_type":"code","source":["# Source: https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/image_segmentation/semantic_segmentation_unet\n","\n","class TeethSegmentationDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = os.listdir(image_dir)\n","        self.masks = os.listdir(mask_dir)\n","    \n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, self.images[index])\n","        mask_path = os.path.join(self.mask_dir, self.images[index]).replace('JPG', 'jpg')\n","        # print(img_path)\n","        # print(mask_path)\n","        image = np.array(Image.open(img_path))[:832, :1600, :]  # make size divisible by 32 by shaving off 8 pixels\n","        mask = np.array(Image.open(mask_path))[:832, :1600, :]\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, mask"],"metadata":{"id":"8xKYBeXkfwi1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: finding mean and standard deviation of dataset"],"metadata":{"id":"h46IvOGIN0gQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trans = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n","])\n","data = TeethSegmentationDataset(image_dir=IMAGES_PATH, mask_dir=MASKS_PATH, transform=trans)\n","train_data, val_data, test_data = random_split(data, [800, 100, 100], generator=torch.Generator().manual_seed(42))\n"],"metadata":{"id":"2M_6yOw_Y7hI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# display an example of an image and a mask\n","image, mask = train_data[42]\n","print(image.shape)\n","# transpose because Image expects (H, W, C)\n","image_transform = torchvision.transforms.ToPILImage()\n","image = image_transform(image)\n","mask = image_transform(mask)\n","display(image)\n","display(mask)"],"metadata":{"id":"Rfb7XgOHduFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_data))\n","print(len(val_data))\n","print(len(test_data))\n","\n","# expect 800, 100, 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHaTw2pwhP7X","executionInfo":{"status":"ok","timestamp":1653962629223,"user_tz":420,"elapsed":146,"user":{"displayName":"Poojan Pandya","userId":"12302368320074108368"}},"outputId":"8b6d261e-33d5-4f56-c63f-60cd0f3437da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["800\n","100\n","100\n"]}]},{"cell_type":"code","source":["assert set(test_data.indices).isdisjoint(set(train_data.indices))\n","assert set(test_data.indices).isdisjoint(set(val_data.indices))\n","assert set(train_data.indices).isdisjoint(set(val_data.indices))"],"metadata":{"id":"LOZRNSTSoY-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convrelu(in_channels, out_channels, kernel, padding):\n","  return nn.Sequential(\n","    nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n","    nn.ReLU(inplace=True),\n","  )\n","\n","\n","class ResNetUNet(nn.Module):\n","  def __init__(self, n_class):\n","    super().__init__()\n","\n","    self.base_model = torchvision.models.resnet18(pretrained=True)\n","    self.base_layers = list(self.base_model.children())\n","\n","    self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n","    self.layer0_1x1 = convrelu(64, 64, 1, 0)\n","    self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n","    self.layer1_1x1 = convrelu(64, 64, 1, 0)\n","    self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n","    self.layer2_1x1 = convrelu(128, 128, 1, 0)\n","    self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n","    self.layer3_1x1 = convrelu(256, 256, 1, 0)\n","    self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n","    self.layer4_1x1 = convrelu(512, 512, 1, 0)\n","\n","    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","    self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n","    self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n","    self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n","    self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n","\n","    self.conv_original_size0 = convrelu(3, 64, 3, 1)\n","    self.conv_original_size1 = convrelu(64, 64, 3, 1)\n","    self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n","\n","    self.conv_last = nn.Conv2d(64, n_class, 1)\n","\n","  def forward(self, input):\n","    x_original = self.conv_original_size0(input)\n","    x_original = self.conv_original_size1(x_original)\n","\n","    layer0 = self.layer0(input)\n","    layer1 = self.layer1(layer0)\n","    layer2 = self.layer2(layer1)\n","    layer3 = self.layer3(layer2)\n","    layer4 = self.layer4(layer3)\n","\n","    layer4 = self.layer4_1x1(layer4)\n","    x = self.upsample(layer4)\n","    layer3 = self.layer3_1x1(layer3)\n","    x = torch.cat([x, layer3], dim=1)\n","    x = self.conv_up3(x)\n","\n","    x = self.upsample(x)\n","    layer2 = self.layer2_1x1(layer2)\n","    x = torch.cat([x, layer2], dim=1)\n","    x = self.conv_up2(x)\n","\n","    x = self.upsample(x)\n","    layer1 = self.layer1_1x1(layer1)\n","    x = torch.cat([x, layer1], dim=1)\n","    x = self.conv_up1(x)\n","\n","    x = self.upsample(x)\n","    layer0 = self.layer0_1x1(layer0)\n","    x = torch.cat([x, layer0], dim=1)\n","    x = self.conv_up0(x)\n","\n","    x = self.upsample(x)\n","    x = torch.cat([x, x_original], dim=1)\n","    x = self.conv_original_size2(x)\n","\n","    out = self.conv_last(x)\n","\n","    return out"],"metadata":{"id":"F57IuOTjkl9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('device', device)\n","\n","model = ResNetUNet(2)\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlwOPjfG3uOT","executionInfo":{"status":"ok","timestamp":1653962900020,"user_tz":420,"elapsed":354,"user":{"displayName":"Poojan Pandya","userId":"12302368320074108368"}},"outputId":"abd1ece6-6c9f-4447-9e38-472a1bdea7c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["device cuda\n"]}]},{"cell_type":"code","source":["model"],"metadata":{"id":"EWLukYa43xnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","summary(model, input_size=(3, 832, 1600))"],"metadata":{"id":"zaW_9T9p357n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        \n","        #comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = F.sigmoid(inputs)       \n","        \n","        #flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        intersection = (inputs * targets).sum()                            \n","        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n","        \n","        return 1 - dice"],"metadata":{"id":"KzmZ-VDnizJk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(train_data, batch_size=5, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=5, shuffle=True)\n","dataloaders = {\n","    'train': train_loader,\n","    'val': val_loader\n","}"],"metadata":{"id":"FpwjFyLUUfQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_path = \"checkpoint.pth\"\n","\n","def calc_loss(pred, target, metrics, bce_weight=0.5):\n","    bce = F.binary_cross_entropy_with_logits(pred, target)\n","\n","    pred = torch.sigmoid(pred)\n","    dice = DiceLoss(pred, target)\n","\n","    loss = bce * bce_weight + dice * (1 - bce_weight)\n","\n","    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n","    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n","    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n","\n","    return loss\n","\n","def print_metrics(metrics, epoch_samples, phase):\n","    outputs = []\n","    for k in metrics.keys():\n","        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n","\n","    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n","\n","def train_model(model, optimizer, scheduler, num_epochs=25):\n","    best_loss = 1e10\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        since = time.time()\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            metrics = defaultdict(float)\n","            epoch_samples = 0\n","\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.cuda()\n","                labels = labels.cuda()\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    loss = calc_loss(outputs, labels, metrics)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                epoch_samples += inputs.size(0)\n","\n","            print_metrics(metrics, epoch_samples, phase)\n","            epoch_loss = metrics['loss'] / epoch_samples\n","\n","            if phase == 'train':\n","              scheduler.step()\n","              for param_group in optimizer.param_groups:\n","                  print(\"LR\", param_group['lr'])\n","\n","            # save the model weights\n","            if phase == 'val' and epoch_loss < best_loss:\n","                print(f\"saving best model to {checkpoint_path}\")\n","                best_loss = epoch_loss\n","                torch.save(model.state_dict(), checkpoint_path)\n","\n","        time_elapsed = time.time() - since\n","        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","\n","    print('Best val loss: {:4f}'.format(best_loss))\n","\n","    # load best model weights\n","    model.load_state_dict(torch.load(checkpoint_path))\n","    return model"],"metadata":{"id":"VuEAAB2w4kn5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.modules.linear import Linear\n","from torch.nn.modules.activation import ReLU\n","from torch.optim import lr_scheduler\n","\n","num_class = 2\n","# model = ResNetUNet(num_class).to(device)\n","model = nn.Sequential(\n","    nn.Conv2d(3, 10, kernel_size=3),\n","    nn.ReLU(),\n","    nn.Conv2d(10, 3, kernel_size=3),\n","    nn.ReLU(),\n","    nn.Flatten(),\n","    nn.Linear(832*1600*3, 2, bias=True)\n",").cuda()\n","\n","# # freeze backbone layers\n","# for l in model.base_layers:\n","#   for param in l.parameters():\n","#     param.requires_grad = False\n","\n","optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n","\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=8, gamma=0.1)\n","\n","model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=10)"],"metadata":{"id":"R2uMUgWl5g6Z"},"execution_count":null,"outputs":[]}]}